<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>CRAN - Package tokenizers</title>
<link rel="stylesheet" type="text/css" href="../../CRAN_web.css" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="citation_title" content="Fast, Consistent Tokenization of Natural Language Text [R package tokenizers version 0.2.1]" />
<meta name="citation_author" content="Lincoln Mullen" />
<meta name="citation_publication_date" content="2018-03-29" />
<meta name="citation_public_url" content="https://CRAN.R-project.org/package=tokenizers" />
<meta name="DC.identifier" content="https://CRAN.R-project.org/package=tokenizers" />
<meta name="DC.publisher" content="Comprehensive R Archive Network (CRAN)" />
<meta name="og:title" content="tokenizers: Fast, Consistent Tokenization of Natural Language Text" />
<meta name="og:description" content="Convert natural language text into tokens. Includes tokenizers for shingled n-grams, skip n-grams, words, word stems, sentences, paragraphs, characters, shingled characters, lines, tweets, Penn Treebank, regular expressions, as well as functions for counting characters, words, and sentences, and a function for splitting longer texts into separate documents, each with the same number of words. The tokenizers have a consistent interface, and the package is built on the 'stringi' and 'Rcpp' packages for fast yet correct tokenization in 'UTF-8'. " />
<meta name="og:image" content="https://CRAN.R-project.org/CRANlogo.png" />
<meta name="og:type" content="website" />
<meta name="og:url" content="https://CRAN.R-project.org/package=tokenizers" />
<meta name="twitter:card" content="summary" />
<meta name="twitter:site" content="@_R_Foundation" />
<style type="text/css">
  table td { vertical-align: top; }
</style>
</head>
<body>
<h2>tokenizers: Fast, Consistent Tokenization of Natural Language Text</h2>
<p>Convert natural language text into tokens. Includes tokenizers for
    shingled n-grams, skip n-grams, words, word stems, sentences, paragraphs,
    characters, shingled characters, lines, tweets, Penn Treebank, regular
    expressions, as well as functions for counting characters, words, and sentences,
    and a function for splitting longer texts into separate documents, each with
    the same number of words.  The tokenizers have a consistent interface, and
    the package is built on the 'stringi' and 'Rcpp' packages for  fast
    yet correct tokenization in 'UTF-8'. </p>
<table summary="Package tokenizers summary">
<tr>
<td>Version:</td>
<td>0.2.1</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.1.3)</td>
</tr>
<tr>
<td>Imports:</td>
<td><a href="../stringi/index.html">stringi</a> (&ge; 1.0.1), <a href="../Rcpp/index.html">Rcpp</a> (&ge; 0.12.3), <a href="../SnowballC/index.html">SnowballC</a> (&ge; 0.5.1)</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td><a href="../Rcpp/index.html">Rcpp</a></td>
</tr>
<tr>
<td>Suggests:</td>
<td><a href="../covr/index.html">covr</a>, <a href="../knitr/index.html">knitr</a>, <a href="../rmarkdown/index.html">rmarkdown</a>, <a href="../stopwords/index.html">stopwords</a> (&ge; 0.9.0), <a href="../testthat/index.html">testthat</a></td>
</tr>
<tr>
<td>Published:</td>
<td>2018-03-29</td>
</tr>
<tr>
<td>Author:</td>
<td>Lincoln Mullen <a href="https://orcid.org/0000-0001-5103-6917"><img alt="ORCID iD" src="/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre],
  Os Keyes <a href="https://orcid.org/0000-0001-5196-609X"><img alt="ORCID iD" src="/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [ctb],
  Dmitriy Selivanov [ctb],
  Jeffrey Arnold <a href="https://orcid.org/0000-0001-9953-3904"><img alt="ORCID iD" src="/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [ctb],
  Kenneth Benoit <a href="https://orcid.org/0000-0002-0797-564X"><img alt="ORCID iD" src="/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Lincoln Mullen  &#x3c;&#x6c;&#x69;&#x6e;&#x63;&#x6f;&#x6c;&#x6e;&#x20;&#x61;&#x74;&#x20;&#x6c;&#x69;&#x6e;&#x63;&#x6f;&#x6c;&#x6e;&#x6d;&#x75;&#x6c;&#x6c;&#x65;&#x6e;&#x2e;&#x63;&#x6f;&#x6d;&#x3e;</td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/ropensci/tokenizers/issues">https://github.com/ropensci/tokenizers/issues</a></td>
</tr>
<tr>
<td>License:</td>
<td><a href="../../licenses/MIT">MIT</a> + file <a href="LICENSE">LICENSE</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://lincolnmullen.com/software/tokenizers/">https://lincolnmullen.com/software/tokenizers/</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Citation:</td>
<td><a href="citation.html">tokenizers citation info</a> </td>
</tr>
<tr>
<td>Materials:</td>
<td><a href="README.html">README</a> <a href="news.html">NEWS</a> </td>
</tr>
<tr>
<td>In&nbsp;views:</td>
<td><a href="../../views/NaturalLanguageProcessing.html">NaturalLanguageProcessing</a></td>
</tr>
<tr>
<td>CRAN&nbsp;checks:</td>
<td><a href="../../checks/check_results_tokenizers.html">tokenizers results</a></td>
</tr>
</table>
<h4>Downloads:</h4>
<table summary="Package tokenizers downloads">
<tr>
<td> Reference&nbsp;manual: </td>
<td> <a href="tokenizers.pdf"> tokenizers.pdf </a> </td>
</tr>
<tr>
<td>Vignettes:</td>
<td>
<a href="vignettes/introduction-to-tokenizers.html">Introduction to the tokenizers Package</a><br/>
<a href="vignettes/tif-and-tokenizers.html">The Text Interchange Formats and the tokenizers Package</a><br/>
</td>
</tr>
<tr>
<td> Package&nbsp;source: </td>
<td> <a href="../../../src/contrib/tokenizers_0.2.1.tar.gz"> tokenizers_0.2.1.tar.gz </a> </td>
</tr>
<tr>
<td> Windows&nbsp;binaries: </td>
<td> r-devel: <a href="../../../bin/windows/contrib/4.1/tokenizers_0.2.1.zip">tokenizers_0.2.1.zip</a>, r-release: <a href="../../../bin/windows/contrib/4.0/tokenizers_0.2.1.zip">tokenizers_0.2.1.zip</a>, r-oldrel: <a href="../../../bin/windows/contrib/3.6/tokenizers_0.2.1.zip">tokenizers_0.2.1.zip</a> </td>
</tr>
<tr>
<td> macOS&nbsp;binaries: </td>
<td> r-release: <a href="../../../bin/macosx/contrib/4.0/tokenizers_0.2.1.tgz">tokenizers_0.2.1.tgz</a>, r-oldrel: <a href="../../../bin/macosx/el-capitan/contrib/3.6/tokenizers_0.2.1.tgz">tokenizers_0.2.1.tgz</a> </td>
</tr>
<tr>
<td> Old&nbsp;sources: </td>
<td> <a href="https://CRAN.R-project.org/src/contrib/Archive/tokenizers"> tokenizers archive </a> </td>
</tr>
</table>
<h4>Reverse dependencies:</h4>
<table summary="Package tokenizers reverse dependencies">
<tr>
<td>Reverse&nbsp;imports:</td>
<td><a href="../covfefe/index.html">covfefe</a>, <a href="../DramaAnalysis/index.html">DramaAnalysis</a>, <a href="../epitweetr/index.html">epitweetr</a>, <a href="../healthforum/index.html">healthforum</a>, <a href="../pdfsearch/index.html">pdfsearch</a>, <a href="../proustr/index.html">proustr</a>, <a href="../rslp/index.html">rslp</a>, <a href="../textfeatures/index.html">textfeatures</a>, <a href="../textrecipes/index.html">textrecipes</a>, <a href="../tidypmc/index.html">tidypmc</a>, <a href="../tidytext/index.html">tidytext</a>, <a href="../wactor/index.html">wactor</a></td>
</tr>
<tr>
<td>Reverse&nbsp;suggests:</td>
<td><a href="../cwbtools/index.html">cwbtools</a>, <a href="../edgarWebR/index.html">edgarWebR</a>, <a href="../quanteda/index.html">quanteda</a></td>
</tr>
</table>
<h4>Linking:</h4>
<p>Please use the canonical form
<a href="https://CRAN.R-project.org/package=tokenizers"><samp>https://CRAN.R-project.org/package=tokenizers</samp></a>
to link to this page.</p>
</body>
</html>
