<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>CRAN - Package gradDescent</title>
<link rel="stylesheet" type="text/css" href="../../CRAN_web.css" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="DC.identifier" content="https://CRAN.R-project.org/package=gradDescent" />
<meta name="DC.publisher" content="Comprehensive R Archive Network (CRAN)" />
<meta name="og:title" content="gradDescent: Gradient Descent for Regression Tasks" />
<meta name="og:description" content="An implementation of various learning algorithms based on Gradient Descent for dealing with regression tasks. The variants of gradient descent algorithm are : Mini-Batch Gradient Descent (MBGD), which is an optimization to use training data partially to reduce the computation load. Stochastic Gradient Descent (SGD), which is an optimization to use a random data in learning to reduce the computation load drastically. Stochastic Average Gradient (SAG), which is a SGD-based algorithm to minimize stochastic step to average. Momentum Gradient Descent (MGD), which is an optimization to speed-up gradient descent learning. Accelerated Gradient Descent (AGD), which is an optimization to accelerate gradient descent learning. Adagrad, which is a gradient-descent-based algorithm that accumulate previous cost to do adaptive learning. Adadelta, which is a gradient-descent-based algorithm that use hessian approximation to do adaptive learning. RMSprop, which is a gradient-descent-based algorithm that combine Adagrad and Adadelta adaptive learning ability. Adam, which is a gradient-descent-based algorithm that mean and variance moment to do adaptive learning. Stochastic Variance Reduce Gradient (SVRG), which is an optimization SGD-based algorithm to accelerates the process toward converging by reducing the gradient. Semi Stochastic Gradient Descent (SSGD),which is a SGD-based algorithm that combine GD and SGD to accelerates the process toward converging by choosing one of the gradients at a time. Stochastic Recursive Gradient Algorithm (SARAH), which is an optimization algorithm similarly SVRG to accelerates the process toward converging by accumulated stochastic information. Stochastic Recursive Gradient Algorithm+ (SARAHPlus), which is a SARAH practical variant algorithm to accelerates the process toward converging provides a possibility of earlier termination." />
<meta name="og:image" content="https://CRAN.R-project.org/CRANlogo.png" />
<meta name="og:type" content="website" />
<meta name="og:url" content="https://CRAN.R-project.org/package=gradDescent" />
<meta name="twitter:card" content="summary" />
<meta name="twitter:site" content="@_R_Foundation" />
<style type="text/css">
  table td { vertical-align: top; }
</style>
</head>
<body>
<h2>gradDescent: Gradient Descent for Regression Tasks</h2>
<p>An implementation of various learning algorithms based on Gradient Descent for dealing with regression tasks. 
	The variants of gradient descent algorithm are :
	Mini-Batch Gradient Descent (MBGD), which is an optimization to use training data partially to reduce the computation load.
	Stochastic Gradient Descent (SGD), which is an optimization to use a random data in learning to reduce the computation load drastically.
	Stochastic Average Gradient (SAG), which is a SGD-based algorithm to minimize stochastic step to average.
	Momentum Gradient Descent (MGD), which is an optimization to speed-up gradient descent learning.
	Accelerated Gradient Descent (AGD), which is an optimization to accelerate gradient descent learning.
	Adagrad, which is a gradient-descent-based algorithm that accumulate previous cost to do adaptive learning.
	Adadelta, which is a gradient-descent-based algorithm that use hessian approximation to do adaptive learning.
	RMSprop, which is a gradient-descent-based algorithm that combine Adagrad and Adadelta adaptive learning ability.
	Adam, which is a gradient-descent-based algorithm that mean and variance moment to do adaptive learning.
	Stochastic Variance Reduce Gradient (SVRG), which is an optimization SGD-based algorithm to accelerates the process toward converging by reducing the gradient.
	Semi Stochastic Gradient Descent (SSGD),which is a SGD-based algorithm that combine GD and SGD to accelerates the process toward converging by choosing one of the gradients at a time.
	Stochastic Recursive Gradient Algorithm (SARAH), which is an optimization algorithm similarly SVRG to accelerates the process toward converging by accumulated stochastic information.
	Stochastic Recursive Gradient Algorithm+ (SARAHPlus), which is a SARAH practical variant algorithm to accelerates the process toward converging provides a possibility of earlier termination.</p>
<table summary="Package gradDescent summary">
<tr>
<td>Version:</td>
<td>3.0</td>
</tr>
<tr>
<td>Published:</td>
<td>2018-01-25</td>
</tr>
<tr>
<td>Author:</td>
<td>Galih Praja Wijaya, Dendi Handian, Imam Fachmi Nasrulloh, Lala Septem Riza, Rani Megasari, Enjun Junaeti</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Lala Septem Riza  &#x3c;&#x6c;&#x61;&#x6c;&#x61;&#x2e;&#x73;&#x2e;&#x72;&#x69;&#x7a;&#x61;&#x20;&#x61;&#x74;&#x20;&#x75;&#x70;&#x69;&#x2e;&#x65;&#x64;&#x75;&#x3e;</td>
</tr>
<tr>
<td>License:</td>
<td><a href="../../licenses/GPL-2">GPL-2</a> | <a href="../../licenses/GPL-3">GPL-3</a> | file <a href="LICENSE">LICENSE</a> [expanded from: GPL (&ge; 2) | file LICENSE]</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/drizzersilverberg/gradDescentR">https://github.com/drizzersilverberg/gradDescentR</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>In&nbsp;views:</td>
<td><a href="../../views/MachineLearning.html">MachineLearning</a></td>
</tr>
<tr>
<td>CRAN&nbsp;checks:</td>
<td><a href="../../checks/check_results_gradDescent.html">gradDescent results</a></td>
</tr>
</table>
<h4>Downloads:</h4>
<table summary="Package gradDescent downloads">
<tr>
<td> Reference&nbsp;manual: </td>
<td> <a href="gradDescent.pdf"> gradDescent.pdf </a> </td>
</tr>
<tr>
<td> Package&nbsp;source: </td>
<td> <a href="../../../src/contrib/gradDescent_3.0.tar.gz"> gradDescent_3.0.tar.gz </a> </td>
</tr>
<tr>
<td> Windows&nbsp;binaries: </td>
<td> r-devel: <a href="../../../bin/windows/contrib/4.1/gradDescent_3.0.zip">gradDescent_3.0.zip</a>, r-release: <a href="../../../bin/windows/contrib/4.0/gradDescent_3.0.zip">gradDescent_3.0.zip</a>, r-oldrel: <a href="../../../bin/windows/contrib/3.6/gradDescent_3.0.zip">gradDescent_3.0.zip</a> </td>
</tr>
<tr>
<td> macOS&nbsp;binaries: </td>
<td> r-release: <a href="../../../bin/macosx/contrib/4.0/gradDescent_3.0.tgz">gradDescent_3.0.tgz</a>, r-oldrel: <a href="../../../bin/macosx/el-capitan/contrib/3.6/gradDescent_3.0.tgz">gradDescent_3.0.tgz</a> </td>
</tr>
<tr>
<td> Old&nbsp;sources: </td>
<td> <a href="https://CRAN.R-project.org/src/contrib/Archive/gradDescent"> gradDescent archive </a> </td>
</tr>
</table>
<h4>Linking:</h4>
<p>Please use the canonical form
<a href="https://CRAN.R-project.org/package=gradDescent"><samp>https://CRAN.R-project.org/package=gradDescent</samp></a>
to link to this page.</p>
</body>
</html>
